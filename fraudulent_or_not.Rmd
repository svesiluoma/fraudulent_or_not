---
title: "Fraudulent or not?"
author: "Sari Vesiluoma"
date: "12.11.2019"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
# Ensuring having required libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
library(readr)
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)
if(!require(MASS)) install.packages("MASS", repos = "http://cran.us.r-project.org")
library(MASS)
if(!require(gam)) install.packages("gam", repos = "http://cran.us.r-project.org")
library(gam)
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
library(randomForest)
```

## Introduction

The goal in this project is to learn how to predict a fraudulent financial transaction. The data used here is called Synthetic Financial Datasets for Fraud Detection generated by the PaySim mobile money simulator (https://www.kaggle.com/ntnu-testimon/paysim1).As described on the web page, the dataset is a synthetic one, generated using the simulator called PaySim. It uses aggregated data from a private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour. 

PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The synthetic dataset is scaled down 1/4 of the original dataset.

I have downloaded the dataset from the net (the link above) and I have unzipped it to the same folder where my R script and the rmd file are. Here, I am reading the data from my folder. 
```{r read_data, include=FALSE}
fraud_or_not <- read_csv("PS_20174392719_1491204439457_log.csv")
```

The dataset, here referred with a variable name fraud_or_not, has the following dimensions

```{r read_and_dim, echo=FALSE}
dim(fraud_or_not)
```

Next I will analyse the data and split it to training and test sets. I will use different machine learning algorithms to try to predict which transaction is fraudulent and which not. In this kind of a case the speciality is that the amount of fraudulent transactions is very minor compared to the amount of non-fraudulent transactions, as we will see. 


## Analysis
### Understanding the data

Let's look the data first as is. As can be seen from the summary below, there are e.g. no NA values which would need to be cleaned. Zero values do exist, but those might be correct values needed, because the data seems to include different kind of transactios, where different features are relevant and the others might be zero as a value.

```{r summary}
summary(fraud_or_not)
str(fraud_or_not)
fraud_or_not %>% head()
```

The data has 11 columns which are:

```{r explanations, echo=FALSE, warning = FALSE}
# Showing the feature explanations as a table
explanations <- data.frame(feature = "step", expl = "Maps a unit of time in the real world. 1 step is 1 hour of time. Total steps 744 (30 days simulation).")
explanations <- bind_rows(explanations, data_frame(feature = "type", expl = "CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER."))
explanations <- bind_rows(explanations, data_frame(feature = "amount", expl = "Amount of the transaction in local currency."))
explanations <- bind_rows(explanations, data_frame(feature = "nameOrig", expl = "Customer who started the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "oldbalanceOrg", expl = "Initial balance before the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "newbalanceOrig", expl = "New balance after the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "nameDest", expl = "Customer who is the recipient of the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "oldbalanceDest", expl = "Initial balance recipient before the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "newbalanceDest", expl = "New balance recipient after the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "isFraud", expl = "Transactions made by the fraudulent agents inside the simulation"))
explanations <- bind_rows(explanations, data_frame(feature = "isFlaggedFraud", expl = "An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction."))
kable(explanations, caption = "Explanations of the features")
```

The feature isFraud refers to a fraudulent transaction. The feature isFlaggedFraud refers just to a transaction, which is a suspect for a fraud because it is an attempt to transfer more than 200 000. Normally the amount of fraudulent transactions is very small compared to the non-fraudulent transactions. That is the case here also and the prevalence of a fraud is close to one from thousand transactions. 

```{r prevalence_fraud}
mean(fraud_or_not$isFraud)
```

This means that `r (1-mean(fraud_or_not$isFraud))*100` % of the transactions are non-fraudulent. We would get a very high accuracy if predicting that a transaction is always non-fraudulent. 

The amount of transactions per type is described at the next histogram. The biggest amount being CASH_OUT transactions and the smallest amount being DEBIT transactions. 

```{r amount_transactions}
fraud_or_not %>% ggplot(aes(type , fill = type)) + geom_bar()
```

More interesting is actually how the fraudulent transactions are positioned among the transactions. As can be seen from the next histogram, only the categories CASH_OUT and TRANSFER have fraudulent transactions. 

```{r fraudulent_trtypes}
fraud_or_not %>% filter(isFraud==1) %>% 
  ggplot(aes(type , fill = type)) + geom_bar()
```

Because we know that fraudulent transactions are present only in these two transaction type categories, we will remove the data of all other categories to make processing smoother.

```{r remove}
fraud_or_not <- fraud_or_not %>% filter(type == "CASH_OUT" | type == "TRANSFER")
table(fraud_or_not$type)
```

### Training and test sets

For the prediction we need training and test sets. In this project those were created based on the fraud_or_not data the following way, checking that the dimensions of the resulting sets are correct and that the prevalence of fraudulent transactions is similar between the cases. 
```{r train_and_test}
# Creating training and test sets
set.seed(1234, sample.kind = "Rounding")
test_index <- createDataPartition(y = fraud_or_not$isFraud, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- fraud_or_not[-test_index,]
test_set <- fraud_or_not[test_index,]
# Checking the dimensions and prevalence of fraud in these sets
dim(train_set)
mean(train_set$isFraud == "1")
dim(test_set)
mean(test_set$isFraud == "1")
```

### Machine learning

Now we are ready to start trials with machine learning algorithms. To start with, we should get a pretty accurate results if guessing that none of the transactions are fraudulent. Let's try. 

```{r all_nonfraud}
# Algorithm 1: quess that none of the transactions are fraud
# Splitting the training and test data to X and y to make it easier to use those
y_train <- factor(train_set$isFraud)
X_train <- train_set[,which(names(train_set) != "isFraud")]
y_test <- factor(test_set$isFraud)
X_test <- test_set[,which(names(test_set) != "isFraud")]
# Define mu_hat as a predition of no fraudulent transactions
mu_hat <- factor(replicate(length(y_test), 0))
# Check the results with a confusionMatrix - ensure the same levels to be used
cm <- confusionMatrix(data = mu_hat, reference = y_test)
cm
# Store the results of this algorithm
acc <- cm$overall[['Accuracy']]
specif <- sensitivity(mu_hat, y_test, positive= "1")
algorithm_results <- data.frame(method="1: Assume none is fraud", 
                                accuracy=acc, specificity=specif)
```

As we can see from the confusion matrix, the accuracy is really high `r cm$overall['Accuracy']` but, the specificity is 0. In practice, this means that we have failed in predicting any of the frauds. It is important to notice, that in addition to following accuracy getting better, we have to improve radically the specificity, meaning that we will have correctly predicted the fraudulent actions being fraudulent. 

Next, let's look what kind of a result we will gain using logistic regression. 






## Results


## Conclusion