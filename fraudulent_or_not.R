###########################
### Fraudulent or not   ###
### Sari Vesiluoma 2019 ###
###########################

# Ensuring having required libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
library(readr)
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)
if(!require(MASS)) install.packages("MASS", repos = "http://cran.us.r-project.org")
library(MASS)

# Synthetic Financial Datasets For Fraud Detection
# Synthetic datasets generated by the PaySim mobile money simulator
# https://www.kaggle.com/ntnu-testimon/paysim1
# https://www.kaggle.com/ntnu-testimon/paysim1/download#PS_20174392719_1491204439457_log.csv
# The data file is also available in my Github along with the code: https://github.com/svesiluoma/fraudulent_or_not 

# Read the dataset from unzipped file at the same folder where this R script is (available in my GitHub)
fraud_or_not <- read_csv("PS_20174392719_1491204439457_log.csv")

# What are the dimensions of this data?
dim(fraud_or_not)

# Look the summary,structure and first lines of this data
summary(fraud_or_not)
str(fraud_or_not)
fraud_or_not %>% head()

# Showing the feature explanations as a table
explanations <- data.frame(feature = "step", expl = "Maps a unit of time in the real world. 1 step is 1 hour of time. Total steps 744 (30 days simulation).")
explanations <- bind_rows(explanations, data_frame(feature = "type", expl = "CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER."))
explanations <- bind_rows(explanations, data_frame(feature = "amount", expl = "Amount of the transaction in local currency."))
explanations <- bind_rows(explanations, data_frame(feature = "nameOrig", expl = "Customer who started the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "oldbalanceOrg", expl = "Initial balance before the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "newbalanceOrig", expl = "New balance after the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "nameDest", expl = "Customer who is the recipient of the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "oldbalanceDest", expl = "Initial balance recipient before the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "newbalanceDest", expl = "New balance recipient after the transaction"))
explanations <- bind_rows(explanations, data_frame(feature = "isFraud", expl = "Transactions made by the fraudulent agents inside the simulation"))
explanations <- bind_rows(explanations, data_frame(feature = "isFlaggedFraud", expl = "An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction."))
kable(explanations, caption = "Explanations of the features")

#########################################
### Study the specialties of the data ###
#########################################

# Studying nameOrig and nameDest - potential problem: too many values to be treated as and factor
n_distinct(fraud_or_not$nameOrig)
n_distinct(fraud_or_not$nameDest)

# nameOrig distinct nearly as many as all -> remove
fraud_or_not <- subset(fraud_or_not, select = -c(nameOrig))
str(fraud_or_not)

# nameDest quite many values to be treated as a factor - what if using only the first letter?
fraud_or_not <- fraud_or_not %>% mutate(dest = str_sub(nameDest, 1, 1))
n_distinct(fraud_or_not$dest)

# dest has two values, so using it and dropping the nameDest not to have too many factors
fraud_or_not <- subset(fraud_or_not, select = -c(nameDest))

# What amount of the rows are fraudulent
mean(fraud_or_not$isFraud)

# Percentage of non-fraudulent transactions
(1-mean(fraud_or_not$isFraud))*100

# Amount of transactions per type as histogram
fraud_or_not %>% ggplot(aes(type , fill = type)) + geom_bar()

# Amount of fraudulent transactions per type as histogram
fraud_or_not %>% filter(isFraud==1) %>% 
  ggplot(aes(type , fill = type)) + geom_bar()

# Clearing away all other types than CASH_OUT and TRANSFER because in the other there is no fraudulent transactions
fraud_or_not <- fraud_or_not %>% filter(type == "CASH_OUT" | type == "TRANSFER")

# dest having only one variable anymore -> remove
table(fraud_or_not$dest)
fraud_or_not <- subset(fraud_or_not, select = -c(dest))
str(fraud_or_not)

# Replacing chr type with numeric field typeNbr, where TRANSFER = 1 and CASH_OUT = 2
fraud_or_not <- fraud_or_not %>% mutate(typeNbr = ifelse(type == "TRANSFER", 1, 2))
# Removing the type field
fraud_or_not <- subset(fraud_or_not, select = -c(type))
str(fraud_or_not)

# Data still too big to be efficiently processed.
# Amount of fraudulent transactions - all to be included
reduced_set <- fraud_or_not %>% filter(isFraud == 1)
dim(reduced_set)
# Amount of non-fraud
non_fraud <- fraud_or_not %>% filter(isFraud == 0)
dim(non_fraud)

#  a Select a sample of 100000 from the non_fraud
set.seed(14, sample.kind = "Rounding")
non_fraud_selected <- sample_n(non_fraud, 100000)
dim(non_fraud_selected)

# Combine the selected non-fraud to the reduced set
reduced_set <- rbind(reduced_set, non_fraud_selected)
dim(reduced_set)

# Check the reduced 
str(reduced_set)

#########################
### Train & test sets ###
#########################

# Creating training and test sets - to have big enough temp to have big enough test_set
set.seed(28, sample.kind = "Rounding")
test_index <- createDataPartition(y = reduced_set$isFraud, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- reduced_set[-test_index,]
test_set <- reduced_set[test_index,]

# Checking the dimensions and prevalence of fraud in these sets
dim(train_set)
mean(train_set$isFraud == "1")
dim(test_set)
mean(test_set$isFraud == "1")

########################
### Machine learning ###
########################

# Algorithm 1: quess that none of the transactions are fraud
# Splitting the training and test data to X and y to make it easier to use those
y_train <- as.factor(train_set$isFraud)
X_train <- as.data.frame(train_set[,which(names(train_set) != "isFraud")])
y_test <- as.factor(test_set$isFraud)
X_test <- as.data.frame(test_set[,which(names(test_set) != "isFraud")])

# Define mu_hat as a predition of no fraudulent transactions
mu_hat <- factor(replicate(length(y_test), 0))
# Changing the levels of mu_hat to match with y_test
levels(mu_hat) <- levels(y_test)

# Check the results with a confusionMatrix - ensure the same levels to be used
cm <- confusionMatrix(data = mu_hat, reference = y_test)
cm
# Store the results of this algorithm
acc <- cm$overall[['Accuracy']]
specif <- sensitivity(mu_hat, y_test, positive= "1")
algorithm_results <- data.frame(method="1: Assume none is fraud", 
                                accuracy=acc, specificity=specif)

# Algorithm 2: Logistic regression model
# Training & predicting
train_glm <- train(X_train, y_train, method = "glm")
glm_preds <- factor(predict(train_glm, X_test))
# Define accuracy and specificity
cm <- confusionMatrix(data = glm_preds,reference = y_test)
acc <- cm$overall[['Accuracy']]
acc
specif <- sensitivity(glm_preds, y_test, positive= "1")
specif

# Store the results of this algorithm
algorithm_results <- bind_rows(algorithm_results, 
                               data_frame(method="2: Logistic regression", 
                                accuracy=acc, 
                                specificity=specif))

# Algorithm 3: LDA
# Training & predicting
train_lda <- train(X_train, y_train, method = "lda")
lda_preds <- factor(predict(train_lda, X_test))
# Define accuracy and specificity
cm <- confusionMatrix(data = lda_preds,reference = y_test)
acc <- cm$overall[['Accuracy']]
specif <- sensitivity(lda_preds, y_test, positive= "1")
# Store the results of this algorithm
algorithm_results <- bind_rows(algorithm_results, 
                               data_frame(method="3: LDA", 
                                          accuracy=acc, 
                                          specificity=specif))

# Algorithm 4: K-nearest neighbors
# Training & predicting - trials with k values from 3 to 21 (step: 2)
set.seed(1234, sample.kind = "Rounding")
# Original tuning <- data.frame(k=seq(3, 21, 2))
tuning <- data.frame(k=seq(9, 13, 2))
train_knn <- train(X_train, y_train, method = "knn", tuneGrid = tuning)
train_knn$bestTune
knn_preds <- factor(predict(train_knn, X_test))
# Define accuracy and specificity
cm <- confusionMatrix(data = knn_preds,reference = y_test)
acc <- cm$overall[['Accuracy']]
specif <- sensitivity(knn_preds, y_test, positive= "1")
# Store the results of this algorithm
algorithm_results <- bind_rows(algorithm_results, 
                               data_frame(method="4: knn", 
                                          accuracy=acc, 
                                          specificity=specif))

# Results over all algorithms
algorithm_results
